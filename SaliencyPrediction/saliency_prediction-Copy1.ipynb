{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-sight",
   "metadata": {},
   "source": [
    "# Saliency Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-immigration",
   "metadata": {},
   "source": [
    "The task is to create a model that effectively predicts fixation points on a visual scene.\n",
    "(This should not be confused with the saliency detection task, which is an extraction of areas on a visual scene, that are relevant to a given task.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-static",
   "metadata": {},
   "source": [
    "We use the CAT2000 dataset, which, as the authors say, is composed to create better models for saliency prediction. It diversified the then datasets with better resolution data, various categories, and of course more data. This collection consists of 4000 photos, 2000/2000 training/test, which are divided into 20 equinumerous categories such as Action, Social, Line drawings, Objects, etc. Along with the photos are attached, maps of fixation, which represent what parts of the presented images these people watched. A description of the aforementioned dataset can be found under  https://arxiv.org/pdf/1505.03581.pdf\n",
    "\n",
    "![alt text](cat2000.png \"Cat2000 Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-prompt",
   "metadata": {},
   "source": [
    "As a model, we decided to implement the model presented in the paper https://arxiv.org/pdf/1609.01064.pdf\n",
    "which is a model of neural networks, with fully convolutional layers, using the features of different layers, in order to efficiently predict saliency maps. This model also uses the concept of prior, which is a way to define regularities in visual perception. In their model, it is fully learned and integrated, at the last stage, with the features of the image extracted.\n",
    "\n",
    "![alt text](mlnet.png \"MLNet Scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-amsterdam",
   "metadata": {},
   "source": [
    "In our experiment, the input is (480, 640) and the output is (60, 80), so we could set batch_size = 16 using a GeForce GTX 1060 Ti 6GB graphics card. We set the training to 100 epoch. We decided to draw one photo from each category, excluding it from the training and validation process, to check after each epoch what the saliency map looks like for this photo. The course of training and the final comparison can be viewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-buying",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data import Cat2000Loader\n",
    "from model import MLNet\n",
    "from loss import ModMSELoss\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loaders\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "# ratio = 1080/1920\n",
    "# width = 320\n",
    "# image_size = (int(width*ratio) , width)\n",
    "image_size = (480, 640)\n",
    "fix_map_size = (image_size[0] // 8, image_size[1] // 8)\n",
    "prior_size = (fix_map_size[0] // 10, fix_map_size[1] // 10)\n",
    "\n",
    "transform1 = Compose([Resize(image_size), ToTensor()])\n",
    "transform2 = Compose([Resize(fix_map_size), ToTensor()])\n",
    "\n",
    "loaders = Cat2000Loader('cat2000', batch_size=16, transform=transform1, target_transform=transform2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "model = MLNet(prior_size)\n",
    "\n",
    "criterion = ModMSELoss(*fix_map_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,weight_decay=0.0005,momentum=0.9,nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/2021-09-16 05:52:44.712130_.basic_model'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-fraction",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(model, criterion, optimizer, loaders)\n",
    "\n",
    "trainer.run_trainer(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-fundamentals",
   "metadata": {},
   "source": [
    "The graph shows that the measure on the validation (Loss) set has a downward trend, so further training (with better computing resources) could give a better final result. The loss function is transferred directly from the one of the author of the paper who posted their implementations, using the Keras library, at https://github.com/marcellacornia/mlnet.\n",
    "The plot starts around the fifth epoch because the model initially returned a zero-filled tensor for some examples, which was a problem when computing said loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "full_path = 'models/' + str(datetime.datetime.now()) + '_' + '.basic_model'\n",
    "torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-boulder",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-florist",
   "metadata": {},
   "source": [
    "We noticed that the model initially returns the specified image features, similar to the saliency detection task. Over time, the result starts to blur as it goes towards the fixation points, and sometimes focuses entirely on listing important features in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ipyplot\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-billion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = lambda x: f'cat2000/selected/{x}/'\n",
    "for category in os.listdir(root('images')):\n",
    "    original_image = Image.open(glob.glob(os.path.join(root('images'), category, '*.jpg'))[0])\n",
    "    fix_map = Image.open(glob.glob(os.path.join(root('maps'), category, '*.jpg'))[0])\n",
    "    print(f'CATEGORY: {category}')\n",
    "    ipyplot.plot_images([original_image, fix_map], ['original image', 'map fixation'], img_width=500)\n",
    "    pred = [Image.open(path) for path in glob.glob(os.path.join(root('maps'), category, 'outputs', '*.jpeg'))]\n",
    "    ipyplot.plot_images([img for i, img in enumerate(pred) if i%10 == 9], 10*np.arange(10)+9, img_width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-homework",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
