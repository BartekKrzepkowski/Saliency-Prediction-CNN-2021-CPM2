{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from torchbearer.cv_utils import DatasetValidationSplitter\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "def padding(img, shape_r=480, shape_c=640, channels=3):\n",
    "    img_padded = np.zeros((shape_r, shape_c, channels), dtype=np.uint8)\n",
    "    if channels == 1:\n",
    "        img_padded = np.zeros((shape_r, shape_c), dtype=np.uint8)\n",
    "\n",
    "    original_shape = img.shape\n",
    "    rows_rate = original_shape[0]/shape_r\n",
    "    cols_rate = original_shape[1]/shape_c\n",
    "\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
    "        img = cv2.resize(img, (new_cols, shape_r))\n",
    "        if new_cols > shape_c:\n",
    "            new_cols = shape_c\n",
    "        img_padded[:, ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols)] = img\n",
    "    else:\n",
    "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
    "        img = cv2.resize(img, (shape_c, new_rows))\n",
    "        if new_rows > shape_r:\n",
    "            new_rows = shape_r\n",
    "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows), :] = img\n",
    "\n",
    "    return img_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-capacity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, imgs_path, fix_maps_path=None, transform=None, target_transform=None):\n",
    "#         self.images = [os.path.join(imgs_path, category,img) for category in os.listdir(imgs_path)\n",
    "#                                  for img in os.listdir(os.path.join(imgs_path, category)) if img.endswith('.jpg')]\n",
    "#         self.maps = [os.path.join(fix_maps_path, category,img) for category in os.listdir(fix_maps_path)\n",
    "#                                  for img in os.listdir(os.path.join(fix_maps_path, category)) if img.endswith('.jpg')] if fix_maps_path else None\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "#         self.norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = cv2.imread(self.images[idx])\n",
    "#         image = padding(image, image_size1, image_size2, 3).astype('float')\n",
    "#         image = np.rollaxis(image, 2, 0)  \n",
    "#         if self.maps:\n",
    "#             fix_map = cv2.imread(self.maps[idx],0)\n",
    "#             fix_map = padding(fix_map, shape_r_gt, shape_c_gt, 1).astype('float')\n",
    "#         if self.transform:\n",
    "# #             print(image.shape)\n",
    "#             image = torch.tensor(image,dtype=torch.float)\n",
    "#             if image.shape[0] == 1:\n",
    "#                 image = image.expand(3,image_size1,image_size2)\n",
    "#             image = self.norm(image)\n",
    "#             if self.maps:\n",
    "#                 fix_map = torch.tensor(fix_map,dtype=torch.float)\n",
    "#                 fix_map = fix_map.repeat(1,8,8)\n",
    "        \n",
    "# #         print(image.shape, fix_map.shape)\n",
    "#         catt = torch.cat([image, fix_map], 0)\n",
    "#         return catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, imgs_path, fix_maps_path=None, transform=None, target_transform=None):\n",
    "        self.images = [os.path.join(imgs_path,img) for img in os.listdir(os.path.join(imgs_path)) if img.endswith('.jpg')]\n",
    "        self.maps = [os.path.join(fix_maps_path,img) for img in os.listdir(os.path.join(fix_maps_path)) if img.endswith('.png')] if fix_maps_path else None\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        if self.maps:\n",
    "            fix_map = Image.open(self.maps[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.expand(3,480,640)\n",
    "            image = self.norm(image)\n",
    "            if self.maps:\n",
    "                fix_map = self.target_transform(fix_map)\n",
    "#                 fix_map = fix_map.repeat(1,8,8)\n",
    "        print(image.shape, fix_map.shape)\n",
    "        catt = torch.cat([image, fix_map], 0)\n",
    "        return catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-stadium",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, imgs_path, fix_maps_path=None, transform=None, target_transform=None):\n",
    "#         self.images = [os.path.join(imgs_path, category,img) for category in os.listdir(imgs_path)\n",
    "#                                  for img in os.listdir(os.path.join(imgs_path, category)) if img.endswith('.jpg')]\n",
    "#         self.maps = [os.path.join(fix_maps_path, category,img) for category in os.listdir(fix_maps_path)\n",
    "#                                  for img in os.listdir(os.path.join(fix_maps_path, category)) if img.endswith('.jpg')] if fix_maps_path else None\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "#         self.norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.images[idx])\n",
    "#         if self.maps:\n",
    "#             fix_map = Image.open(self.maps[idx])\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#             if image.shape[0] == 1:\n",
    "#                 image = image.expand(3,480,640)\n",
    "#             image = self.norm(image)\n",
    "#             if self.maps:\n",
    "#                 fix_map = self.target_transform(fix_map)\n",
    "# #                 fix_map = fix_map.repeat(1,8,8)\n",
    "        \n",
    "# #         print(image.shape, fix_map.shape)\n",
    "#         catt = torch.cat([image, fix_map], 0)\n",
    "#         return catt / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "image_size1 = 480\n",
    "image_size2 = 640\n",
    "shape_r_gt = image_size1 // 8\n",
    "shape_c_gt = image_size2 // 8\n",
    "prior_size = ( int(shape_r_gt / 10) , int(shape_c_gt / 10) )\n",
    "\n",
    "transform1 = transforms.Compose([Resize((shape_r_gt, shape_c_gt)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "transform2 = transforms.Compose([Resize((shape_r_gt, shape_c_gt)), ToTensor()])\n",
    "transform3 = transforms.Compose([Resize((shape_r_gt, shape_c_gt))])\n",
    "\n",
    "root = 'MLNet-Pytorch/'\n",
    "\n",
    "class Cat200Loader:\n",
    "    def __init__(self, root_path, batch_size=4, frac_train_to_be_val=0.2):\n",
    "        self.datasets = {}\n",
    "        self.loaders = {}\n",
    "        imgs_path = lambda x: f'{root_path}/{x}/'\n",
    "        maps_path = lambda x: f'{root_path}/{x}/'\n",
    "        \n",
    "        self.datasets['test'] = CustomImageDataset(imgs_path('test_images'), transform=transform1)\n",
    "        self.datasets['train'] = CustomImageDataset(imgs_path('images'), maps_path('train'), transform=transform1, target_transform=transform2)\n",
    "        self.datasets['val'] = CustomImageDataset(imgs_path('val_images'), maps_path('val'), transform=transform1, target_transform=transform2)\n",
    "        \n",
    "        \n",
    "        self.loaders['train'] = DataLoader(self.datasets['train'], batch_size=batch_size, shuffle = True, pin_memory=True)\n",
    "        self.loaders['val'] = DataLoader(self.datasets['val'], batch_size=batch_size, shuffle = True, pin_memory=True)\n",
    "        self.loaders['test'] = DataLoader(self.datasets['test'], batch_size=batch_size, shuffle = False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Cat200Loader(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_size\n",
    "\n",
    "loaders = Cat200Loader(root)\n",
    "\n",
    "for el in loaders.loaders['train']:\n",
    "#     print(el[:,:-1,:,:].shape, el[:,-1,:,:].unsqueeze(1).shape)\n",
    "    gt = el[:,-1,:120,:160].unsqueeze(1)\n",
    "    break\n",
    "    for y in gt:\n",
    "        plt.imshow(y[0].data.cpu().numpy(),cmap='gray')\n",
    "        plt.show()\n",
    "        print (\"Original\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-enlargement",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified MSE Loss Function\n",
    "class ModMSELoss(torch.nn.Module):\n",
    "    def __init__(self,shape_r_gt,shape_c_gt):\n",
    "        super(ModMSELoss, self).__init__()\n",
    "        self.shape_r_gt = shape_r_gt\n",
    "        self.shape_c_gt = shape_c_gt\n",
    "        \n",
    "    def forward(self, output , label , prior):\n",
    "        prior_size = prior.shape\n",
    "        \n",
    "        output_max1 = torch.max(torch.max(output,2)[0],2)[0].unsqueeze(2).unsqueeze(2).expand(output.shape[0],output.shape[1],self.shape_r_gt,self.shape_c_gt)\n",
    "        output_max = output_max1.clone()\n",
    "        mask = torch.tensor([subt.sum() for subt in output_max]).cuda()\n",
    "        output_max[mask==0] += torch.tensor(0.001)\n",
    "        reg = ( 1.0/(prior_size[0]*prior_size[1]) ) * ( 1 - prior)**2\n",
    "        loss = torch.mean( ((output / output_max) - label)**2 / (1.1 - label) )  +  torch.sum(reg)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified MSE Loss Function\n",
    "class ModMSELoss(torch.nn.Module):\n",
    "    def __init__(self,shape_r_gt,shape_c_gt):\n",
    "        super(ModMSELoss, self).__init__()\n",
    "        self.shape_r_gt = shape_r_gt\n",
    "        self.shape_c_gt = shape_c_gt\n",
    "        \n",
    "    def forward(self, output , label , prior):\n",
    "        prior_size = prior.shape\n",
    "        mask = torch.tensor([subt.sum() for subt in output.clone()])\n",
    "        cover = torch.zeros_like(output).cuda()\n",
    "        cover[mask==0] = 0.0001\n",
    "        output = output + cover\n",
    "        output_max = torch.max(torch.max(output,2)[0],2)[0].unsqueeze(2).unsqueeze(2).expand(output.shape[0],output.shape[1],self.shape_r_gt,self.shape_c_gt)\n",
    "        \n",
    "        reg = ( 1.0/(prior_size[0]*prior_size[1]) ) * ( 1 - prior)**2\n",
    "        loss = torch.mean( ((output / output_max) - label)**2 / (1.1 - label) )  +  torch.sum(reg)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class MLNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,prior_size):\n",
    "        super(MLNet, self).__init__()\n",
    "        # loading pre-trained vgg16 model and         \n",
    "        # removing last max pooling layer\n",
    "        features = list(models.vgg16(pretrained = True).features)[:-1]\n",
    "        \n",
    "        \n",
    "        # making same spatial size\n",
    "        # by calculation :) \n",
    "        # in pytorch there was problem outputing same size in maxpool2d\n",
    "        features[23].stride = 1\n",
    "        features[23].kernel_size = 5\n",
    "        features[23].padding = 2\n",
    "                \n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "#         self.features.requires_grad = False\n",
    "        # adding dropout layer\n",
    "        self.fddropout = nn.Dropout2d(p=0.5)\n",
    "        # adding convolution layer to down number of filters 1280 ==> 64\n",
    "        self.int_conv = nn.Conv2d(1280,64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.pre_final_conv = nn.Conv2d(64,1,kernel_size=(1, 1), stride=(1, 1) ,padding=(0, 0))\n",
    "        # prior initialized to ones\n",
    "        self.prior = nn.Parameter(torch.ones((1,1,prior_size[0],prior_size[1]), requires_grad=True))\n",
    "        \n",
    "        # bilinear upsampling layer\n",
    "        self.bilinearup = torch.nn.UpsamplingBilinear2d(scale_factor=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        results = []\n",
    "        for ii,model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if ii in {16,23,29}:\n",
    "                results.append(x)\n",
    "        \n",
    "        # concat to get 1280 = 512 + 512 + 256\n",
    "        x = torch.cat((results[0],results[1],results[2]),1) \n",
    "        \n",
    "        # adding dropout layer with dropout set to 0.5 (default)\n",
    "        x = self.fddropout(x)\n",
    "        \n",
    "        # 64 filters convolution layer\n",
    "        x = self.int_conv(x)\n",
    "        # 1*1 convolution layer\n",
    "        x = self.pre_final_conv(x)\n",
    "        \n",
    "        upscaled_prior = self.bilinearup(self.prior)\n",
    "        # print (\"upscaled_prior shape: {}\".format(upscaled_prior.shape))\n",
    "\n",
    "        # dot product with prior\n",
    "        x = x * upscaled_prior\n",
    "        x = torch.nn.functional.relu(x,inplace=True)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-bicycle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, loaders):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loaders = loaders\n",
    "        \n",
    "    def run_trainer(self, epochs):\n",
    "        liveloss = PlotLosses()\n",
    "        for epoch in range(epochs):\n",
    "            self.logs = {}\n",
    "            \n",
    "            self.model.train()\n",
    "            self.run_epoch('train', epoch)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.run_epoch('val', epoch)\n",
    "                \n",
    "            liveloss.update(self.logs)\n",
    "            liveloss.send()\n",
    "                \n",
    "    def run_epoch(self, phase, epoch):\n",
    "        running_loss = 0.0\n",
    "        for x in tqdm(self.loaders.loaders[phase]):\n",
    "            x_true, y_true = x[:,:-1,:,:], x[:,1,:shape_r_gt,:shape_c_gt].unsqueeze(1)\n",
    "            x_true, y_true = x_true.to(self.device), y_true.to(self.device)\n",
    "            print(x_true.shape, y_true.shape)\n",
    "            y_pred = self.model(x_true)\n",
    "            print(x_true.shape, y_pred.shape, y_true.shape)\n",
    "            loss = self.criterion(y_pred, y_true, self.model.prior.clone())\n",
    "            if phase == 'train':\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            running_loss += loss.detach() * x_true.size(0)\n",
    "            print(loss.detach())\n",
    "            if phase == 'val':\n",
    "                plt.imshow(x_true[0][0].data.cpu().numpy(),cmap='gray')\n",
    "                plt.show()\n",
    "                plt.imshow(y_pred[0][0].data.cpu().numpy(),cmap='gray')\n",
    "                plt.show()\n",
    "            \n",
    "        epoch_loss = running_loss / len(self.loaders.loaders[phase].dataset)\n",
    "        self.logs[f'{phase}_loss'] = epoch_loss.item()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLNet(prior_size)\n",
    "\n",
    "\n",
    "# freezing Layer\n",
    "last_freeze_layer = 23\n",
    "for i,param in enumerate(model.parameters()):\n",
    "    if i < last_freeze_layer:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    \n",
    "criterion = ModMSELoss(shape_r_gt,shape_c_gt)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,weight_decay=0.0005,momentum=0.9,nesterov=True)\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "loaders = Cat200Loader('cat2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, criterion, optimizer, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-tribute",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.run_trainer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-satisfaction",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# how many pic you want to visualiz at randomly\n",
    "no_visual = 15\n",
    "\n",
    "\n",
    "visual_cnt = 0\n",
    "for x in trainer.loaders.loaders['val']:\n",
    "    print (\"Original\")\n",
    "    x_true, y_true = x[:,:-1,:,:], x[:,1,:60,:80].unsqueeze(1)\n",
    "    x_true = x_true.cuda()\n",
    "    y_pred = model.forward(x_true)\n",
    "    # adding term which were subtracted at pre processing\n",
    "    plt.imshow(x_true[0].data.cpu().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n",
    "    print (\"predicted\")\n",
    "    plt.imshow(y_pred[0].squeeze(0).data.cpu().numpy(),cmap='gray')\n",
    "    plt.show()\n",
    "    print (\"Original\")\n",
    "    plt.imshow(y_true[0][0],cmap='gray')\n",
    "    plt.show()\n",
    "    visual_cnt += 1\n",
    "    if visual_cnt > no_visual:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-fifty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
